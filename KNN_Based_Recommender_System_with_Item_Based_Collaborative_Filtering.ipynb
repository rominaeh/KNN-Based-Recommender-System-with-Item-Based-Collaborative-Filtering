{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f9-NLd4W80O"
      },
      "source": [
        "\n",
        "## Exercise 3: Recommender Systems with KNN\n",
        "\n",
        "### Project Description\n",
        "\n",
        "In this exercise, you will develop a simple recommender system using the K-Nearest Neighbors (KNN) algorithm. The project will involve understanding collaborative filtering techniques, implementing a user-based collaborative filtering model, and evaluating its performance.\n",
        "\n",
        "### Phase 1: Understanding Recommender Systems\n",
        "\n",
        "1. **Part 1: Collaborative Filtering Overview**:\n",
        "   - **User-Based Collaborative Filtering**:\n",
        "     - Explain in simple terms how user-based collaborative filtering works. This method recommends items to a user based on the preferences of similar users.\n",
        "   - **Item-Based Collaborative Filtering**:\n",
        "     - Explain how item-based collaborative filtering differs from user-based filtering. This method recommends items similar to those that the user has liked or interacted with before.\n",
        "\n",
        "2. **Part 2: Similarity Metrics**:\n",
        "   - **Cosine Similarity**:\n",
        "     - Explain what cosine similarity is and how it is used to measure the similarity between two vectors (e.g., user preference vectors or item feature vectors).\n",
        "     - Discuss the significance of cosine similarity in the context of collaborative filtering.\n",
        "\n",
        "### Phase 2: Data Preprocessing\n",
        "\n",
        "Before building the recommender system, preprocess the data to ensure it is in the right format.\n",
        "\n",
        "1. **Part 3: Data Preparation**:\n",
        "   - Download the `ratings.csv` dataset, which contains user-item interactions (e.g., ratings).\n",
        "   - Perform Min-Max Scaling on the ratings to normalize the data between 0 and 1.\n",
        "\n",
        "2. **Part 4: Data Splitting**:\n",
        "   - Split the dataset into training and testing sets to evaluate the model's performance on unseen data.\n",
        "\n",
        "### Phase 3: Model Development\n",
        "\n",
        "1. **Part 5: KNN-Based Collaborative Filtering**:\n",
        "   - Implement a user-based collaborative filtering model using the Surprise library and the KNN algorithm.\n",
        "   - Use cosine similarity as the metric to find similar users.\n",
        "\n",
        "2. **Part 6: Model Training**:\n",
        "   - Train the model on the training data and generate predictions for the test set.\n",
        "\n",
        "### Phase 4: Model Evaluation\n",
        "\n",
        "1. **Part 7: Predictions**:\n",
        "   - Print the first five predictions made by the model for the test set.\n",
        "\n",
        "2. **Part 8: Performance Metrics**:\n",
        "   - Evaluate the model's performance using metrics such as Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE).\n",
        "   - Compare the performance with a baseline model (e.g., a model that predicts the average rating).\n",
        "\n",
        "### Phase 5: Conclusion\n",
        "\n",
        "1. **Part 9: Insights and Interpretation**:\n",
        "   - Discuss the strengths and limitations of the KNN-based recommender system.\n",
        "   - Provide insights into how the model could be improved or extended (e.g., by incorporating item-based filtering or hybrid methods).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxACmU6gj4Cf",
        "outputId": "764cbe4f-0620-439d-8072-f8b9db373290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp310-cp310-linux_x86_64.whl size=2357294 sha256=98cd7c7ca087f14922599860eb6f75a90b43b2e588891d1154a8f17a9e20a5b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/3f/df/6acbf0a40397d9bf3ff97f582cc22fb9ce66adde75bc71fd54\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U_pxm74VAEs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from surprise import Dataset, Reader, KNNBasic, accuracy\n",
        "from surprise.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul7jn8ESjMRC"
      },
      "source": [
        "#EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhvFVVN8VDX8"
      },
      "outputs": [],
      "source": [
        "ratings_df = pd.read_csv('/content/ratings.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR5hXzkcjN1X"
      },
      "source": [
        "#Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia8DWk7KVT9f"
      },
      "outputs": [],
      "source": [
        "# Perform Min-Max Scaling\n",
        "scaler = MinMaxScaler()\n",
        "ratings_df[['rating']] = scaler.fit_transform(ratings_df[['rating']])\n",
        "\n",
        "# Filter out users and items with very few ratings\n",
        "user_counts = ratings_df['userId'].value_counts()\n",
        "item_counts = ratings_df['movieId'].value_counts()\n",
        "valid_users = user_counts[user_counts >= 5].index\n",
        "valid_items = item_counts[item_counts >= 5].index\n",
        "\n",
        "ratings_df = ratings_df[ratings_df['userId'].isin(valid_users) & ratings_df['movieId'].isin(valid_items)]\n",
        "reader = Reader(rating_scale=(ratings_df['rating'].min(), ratings_df['rating'].max()))\n",
        "data = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader)\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfQG_xoojTw1"
      },
      "source": [
        "#Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93GFCm2xWuvs",
        "outputId": "999b7d0a-6766-4d4a-a453-818f8d340578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "User-Based Collaborative Filtering:\n",
            "RMSE: 0.2084\n",
            "MAE:  0.1613\n",
            "User-Based RMSE: 0.20842543613441006\n",
            "User-Based MAE: 0.16125310927118833\n",
            "Estimating biases using als...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Item-Based Collaborative Filtering:\n",
            "RMSE: 0.1954\n",
            "MAE:  0.1492\n",
            "Item-Based RMSE: 0.19539891708550408\n",
            "Item-Based MAE: 0.1491597772391319\n",
            "Baseline RMSE: 0.2279486540519928\n",
            "Baseline MAE: 0.18214491824048465\n"
          ]
        }
      ],
      "source": [
        "# User-Based Collaborative Filtering\n",
        "sim_options_user = {\n",
        "    'name': 'pearson_baseline',\n",
        "    'user_based': True,\n",
        "    'min_k': 5,\n",
        "    'shrinkage': 100\n",
        "}\n",
        "model_user = KNNBasic(sim_options=sim_options_user)\n",
        "model_user.fit(trainset)\n",
        "predictions_user = model_user.test(testset)\n",
        "\n",
        "print(\"User-Based Collaborative Filtering:\")\n",
        "rmse_user = accuracy.rmse(predictions_user)\n",
        "mae_user = accuracy.mae(predictions_user)\n",
        "print(f\"User-Based RMSE: {rmse_user}\")\n",
        "print(f\"User-Based MAE: {mae_user}\")\n",
        "\n",
        "# Item-Based Collaborative Filtering\n",
        "sim_options_item = {\n",
        "    'name': 'pearson_baseline',\n",
        "    'user_based': False,\n",
        "    'min_k': 5,\n",
        "    'shrinkage': 100\n",
        "}\n",
        "model_item = KNNBasic(sim_options=sim_options_item)\n",
        "model_item.fit(trainset)\n",
        "predictions_item = model_item.test(testset)\n",
        "\n",
        "print(\"Item-Based Collaborative Filtering:\")\n",
        "rmse_item = accuracy.rmse(predictions_item)\n",
        "mae_item = accuracy.mae(predictions_item)\n",
        "print(f\"Item-Based RMSE: {rmse_item}\")\n",
        "print(f\"Item-Based MAE: {mae_item}\")\n",
        "\n",
        "# Baseline Model\n",
        "baseline_prediction = ratings_df['rating'].mean()\n",
        "baseline_predictions = [(uid, iid, true_r, baseline_prediction, {}) for (uid, iid, true_r) in testset]\n",
        "\n",
        "baseline_rmse = accuracy.rmse(baseline_predictions, verbose=False)\n",
        "baseline_mae = accuracy.mae(baseline_predictions, verbose=False)\n",
        "print(f\"Baseline RMSE: {baseline_rmse}\")\n",
        "print(f\"Baseline MAE: {baseline_mae}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VR8yH-SMXZs7"
      },
      "source": [
        "Item-Based Collaborative Filtering (better in terms of RMSE and MAE.) > User-Based Collaborative Filtering > Baseline Model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}